{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel tremendously benefits from [Hieu Phungs work on Q-Learning](https://www.kaggle.com/phunghieu/connectx-with-q-learning) and [Keon Kims blog](https://keon.github.io/deep-q-learning/). The only thing I do is trying to understand the machinery behind Deep Q Learning with Keras and Gym. Comments are welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from math import exp, log\n",
    "import random\n",
    "from random import choice\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment\n",
    "\n",
    "The ConnectX environment below allows to play around with the setup in a clean gym style which makes it very easy to interact with current states. In order to train my agent properly, the `switch_side`and `switch_trainer` functions are called whenever we start a new game. Therefore, the agent (hopefully) learns to play on both sides of the board against *negamax* and the *random* opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    \n",
    "    def __init__(self, switch_prob=0.5):\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.switch_prob = switch_prob\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=2, shape=(config.rows,config.columns,1), dtype=np.int)\n",
    "\n",
    "    def switch_side(self):\n",
    "        self.pair = self.pair[::-1]\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "    \n",
    "    def switch_trainer(self):\n",
    "        current_trainer_random = 'random' in self.pair \n",
    "        if current_trainer_random:\n",
    "            self.pair = [None, 'negamax']\n",
    "        else:\n",
    "            self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        if random.uniform(0, 1) < self.switch_prob: # switch side\n",
    "            self.switch_side()\n",
    "        if random.uniform(0, 1) < self.switch_prob: # switch trainer\n",
    "            self.switch_trainer()        \n",
    "        return self.trainer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Agent\n",
    "\n",
    "I am really not an expert in neural nets. Thus, all I do is playing around a bit. The magic in defining the agent as below is happening in the `replay` function: After gathering some experience, a neural network is trained to make sense of the `state`, `action` and `reward` relationship. The `target` is set such that the network aims at minimizing the loss between predicting the reward of the `next_state` and the realized reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, episodes):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.98   # discount rate\n",
    "        self.epsilon = 1.0  # initial exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = exp((log(self.epsilon_min) - log(self.epsilon))/(0.8*episodes)) # reaches epsilon_min after 80% of iterations\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr = 0.001))\n",
    "        return model\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # Exploration\n",
    "            return choice([c for c in range(self.action_size) if state[:,c] == 0])\n",
    "            #when exploring, I allow for \"wrong\" moves to give the agent a chance \n",
    "            #to experience the penalty of choosing full columns\n",
    "            #return choice([c for c in range(self.action_size)])\n",
    "        act_values = self.model.predict(state) # Exploitation\n",
    "        action = np.argmax(act_values[0]) \n",
    "        return action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "\n",
    "Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration. Notable thing here is that I let the agent also learn what a valid move is the hard way (a move is invalid if the agent chooses a column which is already full). After an invalid move the game is over (`done = True`) and I penalize invalid actions hard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 50/40000, epsilon: 1.00, average: 0.31\n",
      "episode: 100/40000, epsilon: 0.99, average: 0.23\n"
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = ConnectX(switch_prob = 0.0)\n",
    "state_size = env.observation_space.shape[1]*env.observation_space.shape[0]\n",
    "action_size = env.observation_space.shape[1]\n",
    "episodes = 40000\n",
    "agent = DQNAgent(state_size, action_size, episodes)\n",
    "#agent.load(\"./connectX-weights_deep.h5\") # load prelearned weights\n",
    "batch_size = 40 # Don't know if this number makes sense\n",
    "\n",
    "# Monitoring devices\n",
    "all_total_rewards = np.empty(episodes)\n",
    "all_avg_rewards = np.empty(episodes)\n",
    "\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    while not done:\n",
    "        # Decide action\n",
    "        action = int(agent.act(np.array([state.board])))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if not done:\n",
    "            reward = 1/42 # default: reward of 0.5 if not done/ 1 if win/ 0 if lost\n",
    "        if done:\n",
    "            if reward == 1: # Won\n",
    "                reward = 1\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -1\n",
    "            else: # Draw\n",
    "                reward = 0\n",
    "        if state.board[action]!=0: # invalid move: hard penalization\n",
    "            reward = -10\n",
    "        agent.memorize(np.array([state.board]), action, reward, np.array([next_state.board]), done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        all_total_rewards[e] = total_rewards\n",
    "        avg_reward = all_total_rewards[max(0, e - 100):e].mean()\n",
    "        all_avg_rewards[e] = avg_reward\n",
    "        if e % 50 == 0 :\n",
    "            agent.save(\"./connectX-weights_deep.h5\")\n",
    "            print(\"episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\".format(e, episodes, agent.epsilon, avg_reward))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Did the agent learn anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_avg_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Avg rewards (50)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agent\n",
    "\n",
    "Only relevant for submission. Somewhat cumbersome procedure because Kaggle does not allow `keras`modules for submission but the below procedure seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=self.state_size, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(self.action_size, activation='linear'))\n",
    "model.load_weights('connectX-weights_new.h5')\n",
    "\n",
    "layers = []\n",
    "\n",
    "# Get all layers' weights\n",
    "for i in range(3):\n",
    "    weights, biases = model.layers[i].get_weights()\n",
    "    layers.extend([weights, biases])\n",
    "\n",
    "fc_layers = list(map(\n",
    "    lambda x: str(list(np.round(x, 5))) \\\n",
    "        .replace('array(', '').replace(')', '') \\\n",
    "        .replace(' ', '') \\\n",
    "        .replace('\\n', '') \\\n",
    "        .replace(',dtype=float32',''),\n",
    "    layers\n",
    "))\n",
    "fc_layers = np.reshape(fc_layers, (-1, 2))\n",
    "\n",
    "# Create the agent\n",
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "\n",
    "'''\n",
    "# Write hidden layers\n",
    "for i, (w, b) in enumerate(fc_layers[:-1]):\n",
    "    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n",
    "    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n",
    "\n",
    "my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n",
    "my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n",
    "my_agent += '''\n",
    "    state = observation.board[:]\n",
    "#    state.append(observation.mark)\n",
    "    out = np.array(state, dtype=np.float32)\n",
    "'''\n",
    "\n",
    "for i in range(len(fc_layers[:-1])):\n",
    "    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n",
    "    my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n",
    "\n",
    "my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n",
    "my_agent += '''\n",
    "    for i in range(configuration.columns):\n",
    "        if observation.board[i] != 0:\n",
    "            out[i] = -1e7\n",
    "\n",
    "    return int(np.argmax(out))\n",
    "    '''\n",
    "\n",
    "with open('submission.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify and Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "My Agent vs. Random Agent: 0.91\n",
      "Random Agent vs. My Agent: 0.14\n",
      "My Agent vs. Negamax Agent: 0.0\n",
      "Negamax Agent vs. My Agent: 1.0\n"
     ]
    }
   ],
   "source": [
    "from submission import my_agent\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.run([my_agent, my_agent])\n",
    "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "# Run multiple episodes to estimate agent's performance.\n",
    "print(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=100)))\n",
    "print(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=100)))\n",
    "print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "25ae4cfc330b4de7a67df0b4a1ab43c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45dc66eccdaa4333b74ed3bcba0a7d71",
       "placeholder": "​",
       "style": "IPY_MODEL_cdf9a8c3a5be414ba96f900d47cbdf34",
       "value": " 10/10 [00:16&lt;00:00,  1.65s/it]"
      }
     },
     "45dc66eccdaa4333b74ed3bcba0a7d71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58cfca17967b4ac981d2c04f5f8a9cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6772f369462346bda9d63319d2f11c20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b3c72630fa8409bb51122bb52849933": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58cfca17967b4ac981d2c04f5f8a9cb7",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8db4ac59dae49cd95defd79240e5451",
       "value": 10
      }
     },
     "8354e2ce782145d392264de757dcd7ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7b3c72630fa8409bb51122bb52849933",
        "IPY_MODEL_25ae4cfc330b4de7a67df0b4a1ab43c0"
       ],
       "layout": "IPY_MODEL_6772f369462346bda9d63319d2f11c20"
      }
     },
     "b8db4ac59dae49cd95defd79240e5451": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "cdf9a8c3a5be414ba96f900d47cbdf34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
