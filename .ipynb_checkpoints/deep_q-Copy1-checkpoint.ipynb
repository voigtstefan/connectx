{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel tremendously benefits from [Hieu Phungs work on Q-Learning](https://www.kaggle.com/phunghieu/connectx-with-q-learning) and [Keon Kims blog](https://keon.github.io/deep-q-learning/). The only thing I do is trying to understand the machinery behind Deep Q Learning with Keras and Gym. Comments are welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from math import exp, log\n",
    "from random import choice, uniform, sample\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment\n",
    "\n",
    "The ConnectX environment below allows to play around with the setup in a clean gym style which makes it very easy to interact with current states. In order to train my agent properly, the `switch_side`and `switch_trainer` functions are called whenever we start a new game. Therefore, the agent (hopefully) learns to play on both sides of the board against *negamax* and the *random* opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    \n",
    "    def __init__(self, switch_prob=0.5):\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.switch_prob = switch_prob\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=2, shape=(config.rows,config.columns,1), dtype=np.int)\n",
    "        self.config = config\n",
    "        \n",
    "    def switch_side(self):\n",
    "        self.pair = self.pair[::-1]\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "    \n",
    "    def switch_trainer(self):\n",
    "        current_trainer_random = 'random' in self.pair \n",
    "        if current_trainer_random:\n",
    "            self.pair = [None, 'negamax']\n",
    "        else:\n",
    "            self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        if uniform(0, 1) < self.switch_prob: # switch side\n",
    "            self.switch_side()\n",
    "        #if uniform(0, 1) < self.switch_prob: # switch trainer\n",
    "        #    self.switch_trainer()        \n",
    "        return self.trainer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Agent\n",
    "\n",
    "I am really not an expert in neural nets. Thus, all I do is playing around a bit. The magic in defining the agent as below is happening in the `replay` function: After gathering some experience, a neural network is trained to make sense of the `state`, `action` and `reward` relationship. The `target` is set such that the network aims at minimizing the loss between predicting the reward of the `next_state` and the realized reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, episodes):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=500)\n",
    "        self.gamma = 0.9   # discount rate\n",
    "        self.epsilon = 0.10  # initial exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = exp((log(self.epsilon_min) - log(self.epsilon))/(0.8*episodes)) # reaches epsilon_min after 80% of iterations\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr = 0.001))\n",
    "        return model\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # Exploration\n",
    "            return choice([c for c in range(self.action_size) if state[:,c] == 0])\n",
    "            #when exploring, I allow for \"wrong\" moves to give the agent a chance \n",
    "            #to experience the penalty of choosing full columns\n",
    "            #return choice([c for c in range(self.action_size)])\n",
    "        act_values = self.model.predict(state) # Exploitation\n",
    "        action = np.argmax(act_values[0]) \n",
    "        return action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "\n",
    "Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration. Notable thing here is that I let the agent also learn what a valid move is the hard way (a move is invalid if the agent chooses a column which is already full). After an invalid move the game is over (`done = True`) and I penalize invalid actions hard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 2\n",
      "episode: 100/10000, epsilon: 0.10, average: 1.50\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 3\n",
      "episode: 200/10000, epsilon: 0.09, average: 1.19\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "episode: 300/10000, epsilon: 0.09, average: 1.74\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "episode: 400/10000, epsilon: 0.09, average: 1.33\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 4\n",
      "episode: 500/10000, epsilon: 0.09, average: 1.62\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 4\n",
      "episode: 600/10000, epsilon: 0.08, average: 1.70\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 1\n",
      "episode: 700/10000, epsilon: 0.08, average: 1.94\n",
      "Invalid Action: Invalid column: 0\n",
      "episode: 800/10000, epsilon: 0.08, average: 1.77\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 2\n",
      "episode: 900/10000, epsilon: 0.08, average: 1.92\n",
      "episode: 1000/10000, epsilon: 0.08, average: 2.06\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "episode: 1100/10000, epsilon: 0.07, average: 1.95\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 3\n",
      "episode: 1200/10000, epsilon: 0.07, average: 1.25\n",
      "episode: 1300/10000, epsilon: 0.07, average: 2.14\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 4\n",
      "episode: 1400/10000, epsilon: 0.07, average: 1.50\n",
      "Invalid Action: Invalid column: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-ac21d49a99db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# What would negamax do?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mneg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kaggle_environments\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnectx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnectx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegamax_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mnegamax_agent\u001b[1;34m(obs, config)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnegamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mcolumn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEMPTY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mnegamax\u001b[1;34m(board, mark, depth)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     (score, _) = negamax(next_board,\n\u001b[1;32m--> 115\u001b[1;33m                                          1 if mark == 2 else 2, depth - 1)\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mnegamax\u001b[1;34m(board, mark, depth)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     (score, _) = negamax(next_board,\n\u001b[1;32m--> 115\u001b[1;33m                                          1 if mark == 2 else 2, depth - 1)\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mnegamax\u001b[1;34m(board, mark, depth)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     (score, _) = negamax(next_board,\n\u001b[1;32m--> 115\u001b[1;33m                                          1 if mark == 2 else 2, depth - 1)\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mnegamax\u001b[1;34m(board, mark, depth)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_board\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     (score, _) = negamax(next_board,\n\u001b[1;32m--> 115\u001b[1;33m                                          1 if mark == 2 else 2, depth - 1)\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mnegamax\u001b[1;34m(board, mark, depth)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# Can win next.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEMPTY\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_win\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmoves\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mis_win\u001b[1;34m(board, column, mark, config, has_played)\u001b[0m\n\u001b[0;32m     53\u001b[0m     return (\n\u001b[0;32m     54\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0minarow\u001b[0m  \u001b[1;31m# vertical.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0minarow\u001b[0m  \u001b[1;31m# horizontal.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0minarow\u001b[0m  \u001b[1;31m# top left diagonal.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0minarow\u001b[0m  \u001b[1;31m# top right diagonal.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\envs\\connectx\\connectx.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(offset_row, offset_column)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minarow\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moffset_row\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moffset_column\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = ConnectX(switch_prob = 0.5)\n",
    "state_size = env.observation_space.shape[1]*env.observation_space.shape[0]\n",
    "action_size = env.observation_space.shape[1]\n",
    "episodes = 10000\n",
    "agent = DQNAgent(state_size, action_size, episodes)\n",
    "agent.load(\"./connectX-weights_deep_negamax.h5\") # load prelearned weights\n",
    "batch_size = 40 # Don't know if this number makes sense\n",
    "\n",
    "# Monitoring devices\n",
    "all_total_rewards = np.empty(episodes)\n",
    "all_avg_rewards = np.empty(episodes)\n",
    "\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    while not done:\n",
    "        # Decide action\n",
    "        action = int(agent.act(np.array([state.board])))\n",
    "        # What would negamax do?\n",
    "        neg = __import__(\"kaggle_environments\").envs.connectx.connectx.negamax_agent(state,env.config)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if not done and int(neg)!=action:\n",
    "            reward = -0.5/42 # default: reward of 0.5 if not done/ 1 if win/ 0 if lost\n",
    "        if done:\n",
    "            if reward == 1: # Won\n",
    "                reward = 1\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -1\n",
    "            else: # Draw\n",
    "                reward = 0\n",
    "        if state.board[action]!=0: # invalid move: hard penalization\n",
    "            reward = -10\n",
    "        agent.memorize(np.array([state.board]), action, reward, np.array([next_state.board]), done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        all_total_rewards[e] = total_rewards\n",
    "        avg_reward = all_total_rewards[max(0, e - 100):e].mean()\n",
    "        all_avg_rewards[e] = avg_reward\n",
    "        if e % 100 == 0 :\n",
    "            agent.save(\"./connectX-weights_deep_negamax.h5\")\n",
    "            print(\"episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\".format(e, episodes, agent.epsilon, avg_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the agent learn anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_avg_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Avg rewards (50)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agent\n",
    "\n",
    "Only relevant for submission. Somewhat cumbersome procedure because Kaggle does not allow `keras`modules for submission but the below procedure seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=state_size, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "model.load_weights('connectX-weights_deep_negamax.h5')\n",
    "\n",
    "layers = []\n",
    "\n",
    "# Get all layers' weights\n",
    "for i in range(3):\n",
    "    weights, biases = model.layers[i].get_weights()\n",
    "    layers.extend([weights, biases])\n",
    "\n",
    "fc_layers = list(map(\n",
    "    lambda x: str(list(np.round(x, 8))) \\\n",
    "        .replace('array(', '').replace(')', '') \\\n",
    "        .replace(' ', '') \\\n",
    "        .replace('\\n', '') \\\n",
    "        .replace(',dtype=float32',''),\n",
    "    layers\n",
    "))\n",
    "fc_layers = np.reshape(fc_layers, (-1, 2))\n",
    "\n",
    "# Create the agent\n",
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "\n",
    "'''\n",
    "# Write hidden layers\n",
    "for i, (w, b) in enumerate(fc_layers[:-1]):\n",
    "    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n",
    "    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n",
    "\n",
    "my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n",
    "my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n",
    "my_agent += '''\n",
    "    state = observation.board[:]\n",
    "#    state.append(observation.mark)\n",
    "    out = np.array(state, dtype=np.float32)\n",
    "'''\n",
    "\n",
    "for i in range(len(fc_layers[:-1])):\n",
    "    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n",
    "    my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n",
    "\n",
    "my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n",
    "my_agent += '''\n",
    "    for i in range(configuration.columns):\n",
    "        if observation.board[i] != 0:\n",
    "            out[i] = -1e7\n",
    "\n",
    "    return int(np.argmax(out))\n",
    "    '''\n",
    "\n",
    "with open('submission_deep_negamax.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify and Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "My Agent vs. Random Agent: 0.82\n",
      "Random Agent vs. My Agent: 0.14\n",
      "My Agent vs. Negamax Agent: 0.0\n"
     ]
    }
   ],
   "source": [
    "from submission import my_agent\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.run([my_agent, my_agent])\n",
    "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "# Run multiple episodes to estimate agent's performance.\n",
    "print(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=50)))\n",
    "print(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=50)))\n",
    "print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "25ae4cfc330b4de7a67df0b4a1ab43c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45dc66eccdaa4333b74ed3bcba0a7d71",
       "placeholder": "​",
       "style": "IPY_MODEL_cdf9a8c3a5be414ba96f900d47cbdf34",
       "value": " 10/10 [00:16&lt;00:00,  1.65s/it]"
      }
     },
     "45dc66eccdaa4333b74ed3bcba0a7d71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58cfca17967b4ac981d2c04f5f8a9cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6772f369462346bda9d63319d2f11c20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b3c72630fa8409bb51122bb52849933": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58cfca17967b4ac981d2c04f5f8a9cb7",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8db4ac59dae49cd95defd79240e5451",
       "value": 10
      }
     },
     "8354e2ce782145d392264de757dcd7ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7b3c72630fa8409bb51122bb52849933",
        "IPY_MODEL_25ae4cfc330b4de7a67df0b4a1ab43c0"
       ],
       "layout": "IPY_MODEL_6772f369462346bda9d63319d2f11c20"
      }
     },
     "b8db4ac59dae49cd95defd79240e5451": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "cdf9a8c3a5be414ba96f900d47cbdf34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
