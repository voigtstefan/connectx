{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel tremendously benefits from [Hieu Phungs work on Q-Learning](https://www.kaggle.com/phunghieu/connectx-with-q-learning) and [Keon Kims blog](https://keon.github.io/deep-q-learning/). The only thing I do is trying to understand the machinery behind Deep Q Learning with Keras and Gym. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from kaggle_environments import evaluate, make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment\n",
    "\n",
    "The ConnectX environment below allows to play around with the setup in a clean gym style which makes it very easy to interact with current states. In order to train my agent properly, the `switch_side`and `switch_trainer` functions are called whenever we start a new game. Therefore, the agent learns to play on both sides of the board against *negamax* and the *random* opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectX(gym.Env):\n",
    "    \n",
    "    def __init__(self, switch_prob=0.5):\n",
    "        self.env = make('connectx', debug=True)\n",
    "        self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "        self.switch_prob = switch_prob\n",
    "        config = self.env.configuration\n",
    "        self.action_space = gym.spaces.Discrete(config.columns)\n",
    "        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n",
    "    \n",
    "    def switch_side(self):\n",
    "        self.pair = self.pair[::-1]\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "    \n",
    "    def switch_trainer(self):\n",
    "        current_trainer_random = 'random' in env.pair \n",
    "        if current_trainer_random:\n",
    "            self.pair = [None, 'negamax']\n",
    "        else:\n",
    "            self.pair = [None, 'random']\n",
    "        self.trainer = self.env.train(self.pair)\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.trainer.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        if random.uniform(0, 1) < self.switch_prob: # switch side\n",
    "            self.switch_side()\n",
    "        if random.uniform(0, 1) < self.switch_prob: # switch trainer\n",
    "            self.switch_trainer()        \n",
    "        return self.trainer.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Agent\n",
    "\n",
    "I am really not an expert in neural nets. Thus, all I do is playing around a bit. The magic in defining the agent as below is happening in the `replay` function: After gathering some experience, a neural network is trained to make sense of the `state`, `action` and `reward` relationship. The `target` is set such that the network aims at minimizing the loss between predicting the reward of the `next_state` and the realized reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.98    # discount rate\n",
    "        self.epsilon = 1.0  # initial exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(200, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # Exploration\n",
    "            #return choice([c for c in range(self.action_size) if state[:,c] == 0])\n",
    "            #when exploring, I allow for \"wrong\" moves to give the agent a chance \n",
    "            #to experience the penalty of choosing full columns\n",
    "            return choice([c for c in range(self.action_size)])\n",
    "        act_values = self.model.predict(state) # Exploitation\n",
    "        action = np.argmax(act_values[0]) \n",
    "        return action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "\n",
    "Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration. Notable thing here is that I let the agent also learn what a valid move is the hard way (a move is invalid if the agent chooses a column which is already full). After an invalid move the game is over (`done = True`) and I penalize invalid actions hard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 4\n",
      "episode: 100/10000, epsilon: 0.91, average: -165.20\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 4\n",
      "episode: 200/10000, epsilon: 0.82, average: -284.40\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "episode: 300/10000, epsilon: 0.74, average: -143.80\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 5\n",
      "episode: 400/10000, epsilon: 0.67, average: -223.80\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 0\n",
      "episode: 500/10000, epsilon: 0.61, average: -263.40\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 0\n",
      "episode: 600/10000, epsilon: 0.55, average: -264.20\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 1\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 5\n",
      "episode: 700/10000, epsilon: 0.50, average: -205.20\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 5\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 0\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 4\n",
      "Invalid Action: Invalid column: 3\n",
      "episode: 800/10000, epsilon: 0.45, average: -181.80\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 3\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 6\n",
      "Invalid Action: Invalid column: 2\n",
      "Invalid Action: Invalid column: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-516e5bdc34c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Decide action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m# default: reward of 0.5 if not done/ 1 if win/ 0 if lost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3cf9bd4ca271>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnone_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m             \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             return [\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m()\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"INACTIVE\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36m__get_actions\u001b[1;34m(self, agents, none_action)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mhas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_callable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                 actions[i] = self.__run_agent(\n\u001b[1;32m--> 441\u001b[1;33m                     self.agents[agent], self.state[i])\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\core.py\u001b[0m in \u001b[0;36m__run_agent\u001b[1;34m(self, agent, state)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\site-packages\\kaggle_environments\\utils.py\u001b[0m in \u001b[0;36mtimeout\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SpecProF\\Anaconda\\Anaco\\envs\\connectx\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = ConnectX()\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "#agent.load(\"./connectX-weights_new.h5\") # load prelearned weights\n",
    "batch_size = 20 # Don't know if this number makes sense\n",
    "episodes = 10000\n",
    "\n",
    "# Monitoring devices\n",
    "all_total_rewards = np.empty(episodes)\n",
    "all_avg_rewards = np.empty(episodes)\n",
    "\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    while not done:\n",
    "        # Decide action\n",
    "        action = int(agent.act(np.array([state.board])))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if not done:\n",
    "            reward = 0 # default: reward of 0.5 if not done/ 1 if win/ 0 if lost\n",
    "        if done:\n",
    "            if reward == 1: # Won\n",
    "                reward = 10\n",
    "            elif reward == 0: # Lost\n",
    "                reward = -10\n",
    "            else: # Draw\n",
    "                reward = 0\n",
    "        if state.board[action]!=0: # invalid move: hard penalization\n",
    "            reward = -1000\n",
    "        agent.memorize(np.array([state.board]), action, reward, np.array([next_state.board]), done)\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        all_total_rewards[e] = total_rewards\n",
    "        avg_reward = all_total_rewards[max(0, e - 50):e].mean()\n",
    "        all_avg_rewards[e] = avg_reward\n",
    "        if e % 100 == 0 :\n",
    "            agent.save(\"./connectX-weights_new.h5\")\n",
    "            print(\"episode: {}/{}, epsilon: {:.2f}, average: {:.2f}\".format(e, episodes, agent.epsilon, avg_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the agent learn anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Avg rewards (50)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the agent\n",
    "\n",
    "Only relevant for submission. Somewhat cumbersome procedure because Kaggle does not allow `keras`modules for submission but the below procedure seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=state_size, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "model.load_weights('connectX-weights_new.h5')\n",
    "\n",
    "layers = []\n",
    "\n",
    "# Get all layers' weights\n",
    "for i in range(3):\n",
    "    weights, biases = model.layers[i].get_weights()\n",
    "    layers.extend([weights, biases])\n",
    "\n",
    "fc_layers = list(map(\n",
    "    lambda x: str(list(np.round(x, 5))) \\\n",
    "        .replace('array(', '').replace(')', '') \\\n",
    "        .replace(' ', '') \\\n",
    "        .replace('\\n', '') \\\n",
    "        .replace(',dtype=float32',''),\n",
    "    layers\n",
    "))\n",
    "fc_layers = np.reshape(fc_layers, (-1, 2))\n",
    "\n",
    "# Create the agent\n",
    "my_agent = '''def my_agent(observation, configuration):\n",
    "    import numpy as np\n",
    "\n",
    "'''\n",
    "# Write hidden layers\n",
    "for i, (w, b) in enumerate(fc_layers[:-1]):\n",
    "    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n",
    "    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n",
    "\n",
    "my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n",
    "my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n",
    "my_agent += '''\n",
    "    state = observation.board[:]\n",
    "#    state.append(observation.mark)\n",
    "    out = np.array(state, dtype=np.float32)\n",
    "'''\n",
    "\n",
    "for i in range(len(fc_layers[:-1])):\n",
    "    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n",
    "    my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n",
    "\n",
    "my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n",
    "my_agent += '''\n",
    "    for i in range(configuration.columns):\n",
    "        if observation.board[i] != 0:\n",
    "            out[i] = -1e7\n",
    "\n",
    "    return int(np.argmax(out))\n",
    "    '''\n",
    "\n",
    "with open('submission.py', 'w') as f:\n",
    "    f.write(my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify and Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "My Agent vs. Random Agent: 0.85\n",
      "Random Agent vs. My Agent: 0.09\n",
      "My Agent vs. Negamax Agent: 0.0\n",
      "Negamax Agent vs. My Agent: 1.0\n"
     ]
    }
   ],
   "source": [
    "from submission import my_agent\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.run([my_agent, my_agent])\n",
    "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n",
    "\n",
    "# Run multiple episodes to estimate agent's performance.\n",
    "print(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=100)))\n",
    "print(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=100)))\n",
    "print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\n",
    "print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "25ae4cfc330b4de7a67df0b4a1ab43c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45dc66eccdaa4333b74ed3bcba0a7d71",
       "placeholder": "​",
       "style": "IPY_MODEL_cdf9a8c3a5be414ba96f900d47cbdf34",
       "value": " 10/10 [00:16&lt;00:00,  1.65s/it]"
      }
     },
     "45dc66eccdaa4333b74ed3bcba0a7d71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58cfca17967b4ac981d2c04f5f8a9cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6772f369462346bda9d63319d2f11c20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b3c72630fa8409bb51122bb52849933": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58cfca17967b4ac981d2c04f5f8a9cb7",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8db4ac59dae49cd95defd79240e5451",
       "value": 10
      }
     },
     "8354e2ce782145d392264de757dcd7ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7b3c72630fa8409bb51122bb52849933",
        "IPY_MODEL_25ae4cfc330b4de7a67df0b4a1ab43c0"
       ],
       "layout": "IPY_MODEL_6772f369462346bda9d63319d2f11c20"
      }
     },
     "b8db4ac59dae49cd95defd79240e5451": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "cdf9a8c3a5be414ba96f900d47cbdf34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
